{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12188124,"sourceType":"datasetVersion","datasetId":7676934},{"sourceId":12188228,"sourceType":"datasetVersion","datasetId":7676995}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q numpy pillow torch torchvision requests scipy matplotlib opencv-python\n!pip install -q git+https://github.com/geetu040/transformers.git@depth-pro-projects#egg=transformers\n!pip install --upgrade pip --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:45:24.221165Z","iopub.execute_input":"2025-06-17T15:45:24.221412Z","iopub.status.idle":"2025-06-17T15:47:18.311279Z","shell.execute_reply.started":"2025-06-17T15:45:24.221393Z","shell.execute_reply":"2025-06-17T15:47:18.310293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL\n","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport cv2\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom IPython.display import display\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom huggingface_hub import hf_hub_download\nfrom torchvision import models\nfrom torchvision.transforms import transforms\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.spatial.distance import cdist\nfrom scipy.io import loadmat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:47:18.313077Z","iopub.execute_input":"2025-06-17T15:47:18.313565Z","iopub.status.idle":"2025-06-17T15:47:27.568298Z","shell.execute_reply.started":"2025-06-17T15:47:18.313540Z","shell.execute_reply":"2025-06-17T15:47:27.567698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:47:27.568983Z","iopub.execute_input":"2025-06-17T15:47:27.569397Z","iopub.status.idle":"2025-06-17T15:47:27.641511Z","shell.execute_reply.started":"2025-06-17T15:47:27.569370Z","shell.execute_reply":"2025-06-17T15:47:27.640704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"manual_image_processor = None\ntry:\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    input_size_common = (1536, 1536) # Common size for models if needed, DepthPro expects this.\n    manual_image_processor = transforms.Compose([\n        transforms.Resize(input_size_common),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ])\n    print(f\"Manual image preprocessor initialized with input size: {input_size_common}.\")\nexcept Exception as e:\n    print(f\"CRITICAL ERROR: Failed to initialize image processor. Please check torchvision installation: {e}\")\n    raise RuntimeError(f\"Failed to initialize image processor: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:47:27.643460Z","iopub.execute_input":"2025-06-17T15:47:27.644314Z","iopub.status.idle":"2025-06-17T15:47:27.667267Z","shell.execute_reply.started":"2025-06-17T15:47:27.644281Z","shell.execute_reply":"2025-06-17T15:47:27.666645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Behaviour\n\n","metadata":{}},{"cell_type":"code","source":"try:\n    from transformers import DepthProConfig, DepthProForDepthEstimation\nexcept ImportError as e:\n    print(f\"Error importing core DepthPro model classes. Ensure 'git+https://github.com/geetu040/transformers.git@depth-pro-projects#egg=transformers' was installed correctly.\")\n    raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:47:27.668062Z","iopub.execute_input":"2025-06-17T15:47:27.668318Z","iopub.status.idle":"2025-06-17T15:47:44.838282Z","shell.execute_reply.started":"2025-06-17T15:47:27.668296Z","shell.execute_reply":"2025-06-17T15:47:44.837500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csrnet_model = None\ndepthpro_model = None\nCSRNET_MODEL_WEIGHTS_PATH = '/kaggle/input/weightfile/pytorch/default/1/model_weights_CSRNetConvNeXT12.pth'\nDEPTHPRO_MODEL_REPO_ID = \"geetu040/DepthPro_Segmentation_Human\"\nDEPTHPRO_MODEL_WEIGHTS_FILENAME = \"model_weights.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:47:44.839156Z","iopub.execute_input":"2025-06-17T15:47:44.839714Z","iopub.status.idle":"2025-06-17T15:47:44.843863Z","shell.execute_reply.started":"2025-06-17T15:47:44.839688Z","shell.execute_reply":"2025-06-17T15:47:44.842948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    # Initialize and Load CSRNet Model\n    if csrnet_model is None:\n        print(f\"Initializing and loading CSRNet model from: {CSRNET_MODEL_WEIGHTS_PATH}\")\n        class CSRNet(nn.Module):\n            def __init__(self):\n                super().__init__()\n                convNext = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n                self.frontend = nn.Sequential(\n                    convNext.features[0:4],\n                    nn.Conv2d(192, 512, kernel_size=1)\n                )\n                self.backend = nn.Sequential(\n                    nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(512, 256, kernel_size=3, padding=2, dilation=2),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(256, 128, kernel_size=3, padding=2, dilation=2),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(128, 64, kernel_size=3, padding=2, dilation=2),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(64, 1, kernel_size=1)\n                )\n            def forward(self, x):\n                x = self.frontend(x)\n                x = self.backend(x)\n                return x\n\n        csrnet_model = CSRNet()\n        csrnet_model = csrnet_model.to(DEVICE)\n        csrnet_model.load_state_dict(torch.load(CSRNET_MODEL_WEIGHTS_PATH, map_location=torch.device('cpu')))\n        csrnet_model.eval()\n        print(f\"CSRNet model loaded successfully on {DEVICE}!\")\n\n    # Initialize and Load DepthPro Model\n    if depthpro_model is None: # Only load if not already loaded\n        print(f\"Initializing and loading DepthPro model: {DEPTHPRO_MODEL_REPO_ID}\")\n\n        config = DepthProConfig(use_fov_model=False)\n        depthpro_model = DepthProForDepthEstimation(config)\n\n        features = config.fusion_hidden_size\n        semantic_classifier_dropout = 0.1\n        num_labels = 1\n        depthpro_model.head.head = nn.Sequential(\n            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(),\n            nn.Dropout(semantic_classifier_dropout),\n            nn.Conv2d(features, features, kernel_size=1),\n            nn.ConvTranspose2d(features, num_labels, kernel_size=2, stride=2, padding=0, bias=True),\n        )\n\n        weights_path = hf_hub_download(repo_id=DEPTHPRO_MODEL_REPO_ID, filename=DEPTHPRO_MODEL_WEIGHTS_FILENAME)\n        depthpro_model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'), weights_only=True))\n\n        depthpro_model = depthpro_model.to(DEVICE)\n        depthpro_model.eval()\n        print(f\"DepthPro model loaded successfully on {DEVICE}!\")\n\nexcept Exception as e:\n    print(f\"CRITICAL ERROR during model loading: {e}\")\n    raise RuntimeError(f\"Failed to load one or more models: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:47:44.844766Z","iopub.execute_input":"2025-06-17T15:47:44.845190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_mask(mask, ax, color=None):\n    if color is None:\n        color = np.array([30/255, 144/255, 255/255, 0.6]) # Default light blue\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef draw_flow(img, flow, step=16):\n    h, w = img.shape[:2]\n    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n    fx, fy = flow[y,x].T\n    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n    lines = np.int32(lines + 0.5)\n    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    cv2.polylines(vis, lines, 0, (0, 255, 0)) # Green lines for flow\n    for (x1, y1), (_x2, _y2) in lines:\n        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n    return vis\ndef flow_to_hsv(flow):\n    # This function converts the flow field into an HSV image\n    # Hue encodes direction, saturation encodes magnitude, Value is max\n    h, w = flow.shape[:2]\n    fx, fy = flow[:,:,0], flow[:,:,1]\n    ang = np.arctan2(fy, fx) + np.pi\n    v = np.sqrt(fx*fx+fy*fy)\n    hsv = np.zeros((h, w, 3), np.uint8)\n    hsv[...,0] = ang*(180/np.pi/2)\n    hsv[...,1] = 255\n    hsv[...,2] = cv2.normalize(v, None, 0, 255, cv2.NORM_MINMAX)\n    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n    return rgb\ndef extract_frame(video_path, frame_number):\n    \"\"\"Extracts a specific frame (0-indexed) from a video.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        return None\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n    ret, frame = cap.read()\n    cap.release()\n    return frame if ret else None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_human_mask_and_density(image_bgr, upsampled_density_map, density_threshold_for_refinement=0.1):\n    \"\"\"\n    Performs DepthPro inference for human segmentation and refines the mask using\n    a predicted density map (e.g., from CSRNet).\n    \"\"\"\n    image_pil = Image.fromarray(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n    W, H = image_pil.size\n\n    # --- DepthPro Inference ---\n    input_tensor_depthpro = manual_image_processor(image_pil).unsqueeze(0).to(DEVICE)\n    inputs_depthpro_dict = {'pixel_values': input_tensor_depthpro}\n\n    with torch.no_grad():\n        model_output_depthpro = depthpro_model(**inputs_depthpro_dict, return_dict=True)\n        output_data_depthpro = model_output_depthpro.predicted_depth\n\n    if output_data_depthpro.dim() == 3 and output_data_depthpro.shape[0] == 1:\n        output_data_depthpro = output_data_depthpro.unsqueeze(1)\n\n    # Upsample DepthPro output to original image size\n    output_upsampled_depthpro = F.interpolate(\n        output_data_depthpro,\n        size=(H, W), # Use original H, W\n        mode='bilinear',\n        align_corners=False\n    ).squeeze()\n\n    output_sigmoid_depthpro = output_upsampled_depthpro.sigmoid()\n    human_mask_from_model_depthpro = (output_sigmoid_depthpro > 0.2).float() # Using 0.2 threshold\n\n    # --- Refine mask using the provided upsampled_density_map ---\n    if not isinstance(upsampled_density_map, np.ndarray):\n        print(f\"Warning: upsampled_density_map is not a NumPy array. Type: {type(upsampled_density_map)}. Using zeros for refinement.\")\n        upsampled_density_map = np.zeros((H, W), dtype=np.float32) # Fallback\n\n    density_mask_tensor = (torch.from_numpy(upsampled_density_map).to(DEVICE) > density_threshold_for_refinement).float()\n    final_human_mask_tensor = torch.max(human_mask_from_model_depthpro, density_mask_tensor)\n\n    human_mask_np = final_human_mask_tensor.cpu().numpy().astype(np.uint8) * 255\n    \n    return human_mask_np, upsampled_density_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MIN_DENSITY_FOR_METRIC = 1e-5 # Minimum density value for a pixel to be considered part of the crowd for metric calculation\nMIN_FLOW_MAGNITUDE_FOR_WEIGHTING = 1e-5 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_density_weighted_flow_metrics(masked_flow, density_map):\n    \"\"\"\n    Calculates density-weighted average speed, dominant direction, and average flow components (dx, dy).\n    Returns: avg_speed, dom_angle_degrees, avg_dx, avg_dy\n    \"\"\"\n    total_weighted_dx = 0.0\n    total_weighted_dy = 0.0\n    total_density_sum = 0.0\n\n    crowd_pixel_indices = np.argwhere(density_map > MIN_DENSITY_FOR_METRIC)\n\n    if len(crowd_pixel_indices) == 0:\n        return 0.0, 0.0, 0.0, 0.0\n\n    for y, x in crowd_pixel_indices:\n        dx, dy = masked_flow[y, x]\n        density_value = density_map[y, x]\n\n        if np.abs(dx) > MIN_FLOW_MAGNITUDE_FOR_WEIGHTING or np.abs(dy) > MIN_FLOW_MAGNITUDE_FOR_WEIGHTING:\n            total_weighted_dx += dx * density_value\n            total_weighted_dy += dy * density_value\n            total_density_sum += density_value\n\n    if total_density_sum == 0:\n        return 0.0, 0.0, 0.0, 0.0\n\n    avg_dx = total_weighted_dx / total_density_sum\n    avg_dy = total_weighted_dy / total_density_sum\n\n    average_speed = np.sqrt(avg_dx**2 + avg_dy**2)\n    dominant_angle_radians = np.arctan2(avg_dy, avg_dx)\n    dominant_angle_degrees = np.degrees(dominant_angle_radians)\n\n    if dominant_angle_degrees < 0:\n        dominant_angle_degrees += 360\n\n    return average_speed, dominant_angle_degrees, avg_dx, avg_dy\n\n\n\ndef calculate_flow_magnitude_variance(masked_flow, mask):\n    \"\"\"\n    Calculates the variance of optical flow magnitudes within the given mask.\n    A high variance suggests non-uniform speeds within the crowd.\n    \"\"\"\n    flow_magnitudes = np.linalg.norm(masked_flow[mask > 0], axis=1)\n    if len(flow_magnitudes) > 1: # Need at least 2 points for variance calculation\n        return np.var(flow_magnitudes)\n    return 0.0 # Return 0 if not enough data points\n\ndef calculate_directional_coherence(masked_flow, mask):\n    \"\"\"\n    Calculates the directional coherence of optical flow within the given mask.\n    High coherence (closer to 1.0) means vectors are well-aligned (ordered movement).\n    Low coherence (closer to 0.0) means chaotic or random movement.\n    \"\"\"\n    valid_flow_vectors = masked_flow[mask > 0] # Extract flow vectors where the mask is active\n    \n    if len(valid_flow_vectors) == 0:\n        return 0.0\n\n    magnitudes = np.linalg.norm(valid_flow_vectors, axis=1)\n    non_zero_magnitudes_idx = magnitudes > 1e-6 # Filter out static or near-static pixels\n    \n    if np.sum(non_zero_magnitudes_idx) == 0:\n        return 0.0 # No actual motion to calculate coherence from\n        \n    unit_vectors = valid_flow_vectors[non_zero_magnitudes_idx] / magnitudes[non_zero_magnitudes_idx, np.newaxis]\n\n    mean_resultant_vector = np.mean(unit_vectors, axis=0) # Average of unit vectors\n    coherence = np.linalg.norm(mean_resultant_vector) # Magnitude of the mean resultant vector\n    return min(1.0, coherence) # Coherence should be between 0 and 1\n\ndef calculate_average_density_in_mask(density_map, mask):\n    \"\"\"\n    Calculates the average density within the given mask.\n    \"\"\"\n    masked_density = density_map[mask > 0]\n    if len(masked_density) > 0:\n        return np.mean(masked_density)\n    return 0.0\n\ndef calculate_enthalpy(masked_flow, density_map):\n    \"\"\"\n    Calculates the density-weighted average motion energy (enthalpy proxy) within the crowd.\n    Higher enthalpy means more overall kinetic energy/disorder in motion.\n    \"\"\"\n    total_weighted_squared_magnitude = 0.0\n    total_density_sum = 0.0\n\n    crowd_pixel_indices = np.argwhere(density_map > 0.01) # Filter by significant density\n\n    if len(crowd_pixel_indices) == 0:\n        return 0.0\n\n    for y, x in crowd_pixel_indices:\n        dx, dy = masked_flow[y, x]\n        density_value = density_map[y, x]\n\n        if np.abs(dx) > 0.01 or np.abs(dy) > 0.01: # Filter out near-zero noise motion\n            magnitude_squared = dx**2 + dy**2\n            total_weighted_squared_magnitude += magnitude_squared * density_value\n            total_density_sum += density_value\n\n    if total_density_sum == 0:\n        return 0.0\n\n    average_weighted_enthalpy = total_weighted_squared_magnitude / total_density_sum\n    return average_weighted_enthalpy\n\ndef calculate_flow_divergence(masked_flow, mask):\n    \"\"\"\n    Calculates the average divergence of the optical flow field within the crowd mask.\n    Positive divergence indicates expansion, negative indicates compression.\n    \"\"\"\n    if np.sum(mask) == 0: # No crowd pixels in mask\n        return 0.0\n\n    u = masked_flow[:, :, 0] # Horizontal component of flow\n    v = masked_flow[:, :, 1] # Vertical component of flow\n\n    # Calculate gradients of u and v using numpy.gradient\n    # du/dx (change in horizontal flow over horizontal distance)\n    # dv/dy (change in vertical flow over vertical distance)\n    du_dx = np.gradient(u, axis=1)\n    dv_dy = np.gradient(v, axis=0)\n\n    divergence_map = du_dx + dv_dy\n\n    # Average divergence only over masked crowd pixels\n    masked_divergence = divergence_map[mask > 0]\n    if len(masked_divergence) > 0:\n        return np.mean(masked_divergence)\n    return 0.0\n\ndef calculate_flow_vorticity(masked_flow, mask):\n    \"\"\"\n    Calculates the average vorticity of the optical flow field within the crowd mask.\n    Measures rotational motion (swirling).\n    \"\"\"\n    if np.sum(mask) == 0: # No crowd pixels in mask\n        return 0.0\n\n    u = masked_flow[:, :, 0] # Horizontal component of flow\n    v = masked_flow[:, :, 1] # Vertical component of flow\n\n    # Calculate gradients\n    # du/dy (change in horizontal flow over vertical distance)\n    # dv/dx (change in vertical flow over horizontal distance)\n    du_dy = np.gradient(u, axis=0)\n    dv_dx = np.gradient(v, axis=1)\n\n    vorticity_map = dv_dx - du_dy # Vorticity for 2D flow\n\n    # Average vorticity only over masked crowd pixels\n    masked_vorticity = vorticity_map[mask > 0]\n    if len(masked_vorticity) > 0:\n        return np.mean(masked_vorticity)\n    return 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_crowd_motion_in_video(video_path,\n                                  output_video_dir=\"/kaggle/working/crowd_analysis_output/\",\n                                  density_threshold_for_refinement=0.2,\n                                  ema_alpha=0.2,\n                                  max_frames_to_process=None,\n                                  frames_per_segment=50): # New parameter for segmented output\n    \"\"\"\n    Analyzes collective crowd motion in a video and extracts a comprehensive set of per-frame motion metrics.\n    Generates an output video visualizing segmentation, optical flow, and density, along with metric overlays.\n    Saves the output video in segments for easier viewing and progress tracking.\n    \"\"\"\n    os.makedirs(output_video_dir, exist_ok=True)\n    video_name_base = os.path.splitext(os.path.basename(video_path))[0]\n\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        return []\n\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    print(f\"Processing video: {video_path}\")\n    print(f\"Resolution: {frame_width}x{frame_height}, FPS: {fps}, Total Frames: {total_frames}\")\n\n    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n    out = None # Initialize video writer to None\n    segment_counter = 0 # Track video segments\n\n    # Helper function to open a new VideoWriter for a segment\n    def open_new_video_writer(segment_idx):\n        segment_output_path = os.path.join(output_video_dir, f\"{video_name_base}_segment_{segment_idx:03d}_motion_features_viz.avi\")\n        print(f\"Opening new video segment writer: {segment_output_path}\")\n        return cv2.VideoWriter(segment_output_path, fourcc, fps, (frame_width * 3, frame_height))\n\n    # Initialize the first video writer\n    out = open_new_video_writer(segment_counter)\n\n    ret, prev_frame_bgr = cap.read()\n    if not ret:\n        print(\"Failed to read first frame.\")\n        cap.release()\n        if out: out.release()\n        return []\n\n    prev_smoothed_density_map = np.zeros((frame_height, frame_width), dtype=np.float32)\n\n    prev_frame_pil_csrnet_input = Image.fromarray(cv2.cvtColor(prev_frame_bgr, cv2.COLOR_BGR2RGB))\n    prev_csrnet_input_tensor = manual_image_processor(prev_frame_pil_csrnet_input).unsqueeze(0).to(DEVICE)\n    \n    with torch.no_grad():\n        prev_predicted_density_map_raw = csrnet_model(prev_csrnet_input_tensor)\n        prev_current_frame_upsampled_density = torch.nn.functional.interpolate(\n            prev_predicted_density_map_raw,\n            size=(frame_height, frame_width),\n            mode='bilinear',\n            align_corners=False).squeeze().cpu().numpy()\n            \n    prev_smoothed_density_map = prev_current_frame_upsampled_density.copy()\n\n    prev_human_mask_np, _ = get_human_mask_and_density(\n        prev_frame_bgr,\n        upsampled_density_map=prev_smoothed_density_map,\n        density_threshold_for_refinement=density_threshold_for_refinement\n    )\n    prev_gray = cv2.cvtColor(prev_frame_bgr, cv2.COLOR_BGR2GRAY)\n\n    frame_idx = 1\n    start_time = time.time()\n    \n    all_frame_features = []\n\n    while True:\n        # Check if max_frames_to_process limit is reached for overall processing\n        if max_frames_to_process is not None and frame_idx > max_frames_to_process:\n            print(f\"Reached overall max_frames_to_process limit ({max_frames_to_process}). Stopping processing.\")\n            break\n\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n\n        if frame_idx % 30 == 0 or frame_idx == total_frames - 1:\n            print(f\"Processing frame {frame_idx}/{total_frames}...\")\n\n        current_frame_pil_csrnet_input = Image.fromarray(cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB))\n        current_csrnet_input_tensor = manual_image_processor(current_frame_pil_csrnet_input).unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            current_predicted_density_map_raw = csrnet_model(current_csrnet_input_tensor)\n            current_frame_upsampled_density = torch.nn.functional.interpolate(\n                current_predicted_density_map_raw,\n                size=(frame_height, frame_width),\n                mode='bilinear',\n                align_corners=False).squeeze().cpu().numpy()\n\n        current_smoothed_density_map = (ema_alpha * current_frame_upsampled_density) + \\\n                                       ((1 - ema_alpha) * prev_smoothed_density_map)\n\n        current_human_mask_np, current_density_map_for_viz = \n            get_human_mask_and_density(\n                frame_bgr,\n                upsampled_density_map=current_smoothed_density_map,\n                density_threshold_for_refinement=density_threshold_for_refinement\n            )\n\n        current_human_mask_bool = current_human_mask_np > 0\n        prev_human_mask_bool = prev_human_mask_np > 0\n\n        combined_mask_for_flow = (prev_human_mask_bool & current_human_mask_bool).astype(np.uint8) * 255\n\n        current_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n        flow = cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n        masked_flow = flow.copy()\n        masked_flow[combined_mask_for_flow == 0] = 0\n\n        # --- EXTRACT ALL MOTION & DENSITY METRICS ---\n        avg_crowd_speed, dominant_crowd_angle, avg_flow_dx, avg_flow_dy = \\\n            calculate_density_weighted_flow_metrics(masked_flow, current_smoothed_density_map)\n            \n        flow_mag_variance = calculate_flow_magnitude_variance(masked_flow, combined_mask_for_flow)\n        \n        directional_coherence = calculate_directional_coherence(masked_flow, combined_mask_for_flow)\n        confusion_index = 1.0 - directional_coherence\n        \n        total_crowd_count = (current_predicted_density_map_raw).sum().item()\n        avg_density_in_mask = calculate_average_density_in_mask(current_smoothed_density_map, current_human_mask_np)\n\n        enthalpy = calculate_enthalpy(masked_flow, current_smoothed_density_map)\n\n        flow_divergence = calculate_flow_divergence(masked_flow, combined_mask_for_flow)\n        flow_vorticity = calculate_flow_vorticity(masked_flow, combined_mask_for_flow)\n\n        # --- STORE ALL FEATURES FOR THIS FRAME ---\n        frame_features = {\n            'video_name': video_name_base,\n            'frame_idx': frame_idx,\n            'avg_crowd_speed': avg_crowd_speed,\n            'dominant_crowd_angle': dominant_crowd_angle,\n            'avg_flow_dx': avg_flow_dx,\n            'avg_flow_dy': avg_flow_dy,\n            'flow_magnitude_variance': flow_mag_variance,\n            'directional_coherence': directional_coherence,\n            'confusion_index': confusion_index,\n            'total_crowd_count': total_crowd_count,\n            'avg_density_in_mask': avg_density_in_mask,\n            'enthalpy': enthalpy,\n            'flow_divergence': flow_divergence,\n            'flow_vorticity': flow_vorticity,\n        }\n        all_frame_features.append(frame_features)\n        # --- END FEATURE EXTRACTION ---\n\n        # 4. Visualization\n        seg_viz = frame_bgr.copy()\n        seg_overlay = np.zeros_like(seg_viz, dtype=np.uint8)\n        seg_overlay[current_human_mask_np > 0] = [30, 144, 255]\n        seg_viz = cv2.addWeighted(seg_viz, 1, seg_overlay, 0.6, 0)\n\n        masked_flow_hsv_viz = flow_to_hsv(masked_flow)\n\n        density_viz = np.zeros_like(frame_bgr)\n        if current_smoothed_density_map.max() > 0:\n            normalized_density = cv2.normalize(current_smoothed_density_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n            density_viz = cv2.applyColorMap(normalized_density, cv2.COLORMAP_JET)\n        else:\n            density_viz = np.zeros_like(frame_bgr)\n\n        combined_frame = np.hstack((seg_viz, masked_flow_hsv_viz, density_viz))\n\n        # Text overlays for all metrics\n        cv2.putText(combined_frame, f\"Video: {video_name_base}\", (frame_width * 2 + 10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Frame: {frame_idx}/{total_frames}\", (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n        cv2.putText(combined_frame, f\"Smoothed Pred Count: {total_crowd_count:.2f}\", (frame_width * 2 + 10, 60),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n        \n        cv2.putText(combined_frame, f\"Avg Speed: {avg_crowd_speed:.2f}\", (frame_width * 2 + 10, 90),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Dominant Dir: {dominant_crowd_angle:.1f} deg\", (frame_width * 2 + 10, 120),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Flow Var: {flow_mag_variance:.2f}\", (frame_width * 2 + 10, 150),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Dir Coherence: {directional_coherence:.2f}\", (frame_width * 2 + 10, 180),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Confusion Index: {confusion_index:.2f}\", (frame_width * 2 + 10, 210),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Enthalpy: {enthalpy:.2f}\", (frame_width * 2 + 10, 240),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Divergence: {flow_divergence:.2f}\", (frame_width * 2 + 10, 270),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n        cv2.putText(combined_frame, f\"Vorticity: {flow_vorticity:.2f}\", (frame_width * 2 + 10, 300),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n\n\n        out.write(combined_frame)\n\n        # Check if it's time to close the current segment and open a new one\n        if (frame_idx % frames_per_segment == 0 and frame_idx < total_frames):\n            print(f\"Segment {segment_counter} complete. Closing video writer.\")\n            out.release()\n            segment_counter += 1\n            out = open_new_video_writer(segment_counter)\n        \n        prev_gray = current_gray\n        prev_human_mask_np = current_human_mask_np\n        prev_smoothed_density_map = current_smoothed_density_map\n        prev_frame_bgr = frame_bgr.copy()\n        frame_idx += 1\n\n    end_time = time.time()\n    print(f\"Video processing finished. Total time: {end_time - start_time:.2f} seconds.\")\n    if out: # Ensure the last video writer is released\n        out.release()\n        print(f\"Final video segment saved in '{output_video_dir}'.\")\n    cv2.destroyAllWindows()\n\n    return all_frame_features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EMOTION_LABELS_MAPPING = {\n#     0: 'nothing_emotion',\n#     1: 'Angry',\n#     2: 'Happy',\n#     3: 'Excited',\n#     4: 'Scared',\n#     5: 'Sad',\n#     6: 'Neutral'\n# }\n\n# # --- Behavior Label Mapping ---\n# BEHAVIOR_LABELS_MAPPING = {\n#     0: 'nothing_behavior',\n#     1: 'Panic',\n#     2: 'Fight',\n#     3: 'Congestion',\n#     4: 'Obstacle or abnormal object',\n#     5: 'Neutral_behavior'\n# }\n\n# # --- Function to parse a single MATLAB .m annotation script ---\n# def parse_matlab_annotation_script(script_file_path):\n#     \"\"\"\n#     Parses a MATLAB .m script file (like dataset_frames_emotion_labeling.m)\n#     to extract video labels.\n    \n#     Args:\n#         script_file_path (str): Full path to the .m script file.\n        \n#     Returns:\n#         dict: A dictionary where keys are video IDs (e.g., '001', '002') and\n#               values are NumPy arrays of labels for that video.\n#               Returns an empty dict if parsing fails or file is not found.\n#     \"\"\"\n#     labels_data = {}\n#     try:\n#         with open(script_file_path, 'r') as f:\n#             lines = f.readlines()\n\n#         current_video_id = None\n#         current_video_labels_length = 0\n#         current_labels_array = None\n\n#         for line in lines:\n#             line = line.strip()\n#             # Skip comments and empty lines\n#             if not line or line.startswith('%'):\n#                 continue\n\n#             # Match lines like: video01 = zeros(1,1680);\n#             zeros_match = re.match(r'video(\\d+) = zeros\\(1,(\\d+)\\);', line)\n#             if zeros_match:\n#                 current_video_id = f\"{int(zeros_match.group(1)):03d}\" # Format as '001', '002'\n#                 current_video_labels_length = int(zeros_match.group(2))\n#                 current_labels_array = np.zeros(current_video_labels_length, dtype=int)\n#                 labels_data[current_video_id] = current_labels_array\n#                 continue\n\n#             # Match lines like: video01(1,1:1175)=6;\n#             assignment_match = re.match(r'video(\\d+)\\(1,(\\d+):(\\d+)\\)=(\\d+);', line)\n#             if assignment_match and current_labels_array is not None:\n#                 video_id_assigned = f\"{int(assignment_match.group(1)):03d}\"\n#                 start_frame = int(assignment_match.group(2))\n#                 end_frame = int(assignment_match.group(3))\n#                 label_value = int(assignment_match.group(4))\n\n#                 if video_id_assigned == current_video_id:\n#                     # Adjust to 0-based indexing for Python slicing\n#                     current_labels_array[start_frame - 1 : end_frame] = label_value\n#                 else:\n#                     print(f\"Warning: Assignment for {video_id_assigned} found, but current_video_id is {current_video_id}. Data might be misaligned.\")\n#                 continue\n            \n#             # Match lines like: emotionlabels{1} = video01; or behavelabels{1} = video01;\n#             # These lines associate the videoXX array with the global cell array.\n#             # We don't need to parse these for direct label extraction, as we're building the videoXX arrays directly.\n            \n#     except FileNotFoundError:\n#         print(f\"Error: Script file not found at {script_file_path}\")\n#     except Exception as e:\n#         print(f\"Error parsing script {script_file_path}: {e}\")\n    \n#     return labels_data\n\n# # --- Helper function to get labels for a specific video from parsed data ---\n# def get_labels_for_video_id(video_id, label_type_key, parsed_labels_data):\n#     \"\"\"\n#     Retrieves the label array for a specific video ID and label type\n#     from the globally parsed MATLAB script data.\n    \n#     Args:\n#         video_id (str): The 3-digit video ID (e.g., '001').\n#         label_type_key (str): Key for the parsed data (e.g., 'emotion' or 'behavior').\n#         parsed_labels_data (dict): The global dictionary containing all parsed labels.\n        \n#     Returns:\n#         np.ndarray: A 1D numpy array of numerical labels for that video,\n#                     or None if the video ID or label type is not found.\n#     \"\"\"\n#     if label_type_key in parsed_labels_data and video_id in parsed_labels_data[label_type_key]:\n#         return parsed_labels_data[label_type_key][video_id]\n#     print(f\"Warning: Labels for video {video_id} ({label_type_key}) not found in parsed data.\")\n#     return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     # Define dataset paths\n#     dataset_base_path = \"/kaggle/input/motion-dataset/\" # <--- **UPDATE THIS PATH to your dataset root**\n#     video_dir = os.path.join(dataset_base_path, \"Motion_Emotion_Dataset\")\n    \n#     # Corrected path based on your provided folder structure\n#     annotation_scripts_dir = os.path.join(dataset_base_path, \"Motion Emotion Dataset(MED) annotation\") \n\n#     # Paths to your specific .m annotation script files\n#     emotion_script_path = os.path.join(annotation_scripts_dir, \"dataset_frames_emotion_labeling.m\")\n#     behavior_script_path = os.path.join(annotation_scripts_dir, \"dataset_frames_abnormal_labeling.m\") # Assuming abnormal = behavior\n\n#     kaggle_output_base_dir = \"/kaggle/working/\" # Output directory for generated video and CSV\n#     output_analysis_videos_dir = os.path.join(kaggle_output_base_dir, \"crowd_analysis_outputs\")\n#     os.makedirs(output_analysis_videos_dir, exist_ok=True) # Ensure output directory exists\n\n#     print(\"\\n--- Starting Full Feature Extraction and Dataset Creation ---\")\n\n#     # --- Step 1: Parse MATLAB .m annotation scripts once ---\n#     print(f\"Parsing emotion labels from: {emotion_script_path}\")\n#     parsed_emotion_labels_data = parse_matlab_annotation_script(emotion_script_path)\n#     print(f\"Parsed {len(parsed_emotion_labels_data)} videos for emotion labels.\")\n\n#     print(f\"Parsing behavior labels from: {behavior_script_path}\")\n#     parsed_behavior_labels_data = parse_matlab_annotation_script(behavior_script_path)\n#     print(f\"Parsed {len(parsed_behavior_labels_data)} videos for behavior labels.\")\n\n#     global_parsed_labels = {\n#         'emotion': parsed_emotion_labels_data,\n#         'behavior': parsed_behavior_labels_data\n#     }\n    \n#     all_videos_features = [] # This list will store features from all processed videos\n\n#     # Discover all video files in the specified directory (assuming .mp4 format)\n#     video_files = sorted([f for f in os.listdir(video_dir) if f.endswith('.mp4')])\n    \n#     # --- OPTIONAL: Limit the number of videos processed for quick testing ---\n#     # max_videos_to_process = 2 # Uncomment and set a small number (e.g., 2) for testing\n#     # if 'max_videos_to_process' in locals() and max_videos_to_process is not None:\n#     #     video_files = video_files[:max_videos_to_process]\n    \n#     for video_file_name in video_files:\n#         full_video_path = os.path.join(video_dir, video_file_name)\n#         video_id = os.path.splitext(video_file_name)[0] # Extracts '001' from '001.mp4'\n\n#         # Check if video file exists\n#         if not os.path.exists(full_video_path):\n#             print(f\"Skipping: Video file '{video_file_name}' not found at '{full_video_path}'.\")\n#             continue\n\n#         print(f\"\\nProcessing video: {video_file_name}\")\n        \n#         # --- Call the main analysis function to extract per-frame features ---\n#         per_frame_features = analyze_crowd_motion_in_video(\n#             video_path=full_video_path,\n#             output_video_dir=output_analysis_videos_dir,\n#             density_threshold_for_refinement=0.2,\n#             ema_alpha=0.2,\n#             max_frames_to_process=None # Set to None for full video, or e.g., 50 for a quick test\n#         )\n        \n#         # --- Retrieve Labels for the current video from parsed data ---\n#         emotion_labels_array = get_labels_for_video_id(video_id, 'emotion', global_parsed_labels)\n#         behavior_labels_array = get_labels_for_video_id(video_id, 'behavior', global_parsed_labels)\n        \n#         # Proceed only if features are extracted AND both label arrays are successfully retrieved\n#         if per_frame_features and emotion_labels_array is not None and behavior_labels_array is not None:\n#             # Iterate through the extracted features and add the corresponding labels\n#             for feature_dict in per_frame_features:\n#                 frame_idx_0_based = feature_dict['frame_idx'] - 1 # Convert 1-based frame_idx to 0-based for array indexing\n                \n#                 # Add Emotion Labels\n#                 if frame_idx_0_based < len(emotion_labels_array):\n#                     num_emotion_label = int(emotion_labels_array[frame_idx_0_based])\n#                     feature_dict['emotion_label_numeric'] = num_emotion_label\n#                     feature_dict['emotion_label_str'] = EMOTION_LABELS_MAPPING.get(num_emotion_label, 'Unknown_Emotion')\n#                 else:\n#                     feature_dict['emotion_label_numeric'] = -1\n#                     feature_dict['emotion_label_str'] = 'No_Emotion_Label_Parsed' # Indicate missing label from parsed data\n\n#                 # Add Behavior Labels\n#                 if frame_idx_0_based < len(behavior_labels_array):\n#                     num_behavior_label = int(behavior_labels_array[frame_idx_0_based])\n#                     feature_dict['behavior_label_numeric'] = num_behavior_label\n#                     feature_dict['behavior_label_str'] = BEHAVIOR_LABELS_MAPPING.get(num_behavior_label, 'Unknown_Behavior')\n#                 else:\n#                     feature_dict['behavior_label_numeric'] = -1\n#                     feature_dict['behavior_label_str'] = 'No_Behavior_Label_Parsed' # Indicate missing label from parsed data\n\n#             all_videos_features.extend(per_frame_features) # Add features from current video to master list\n#             print(f\"Finished processing {video_file_name}. Extracted {len(per_frame_features)} frames.\")\n#         else:\n#             print(f\"Skipping {video_file_name} due to missing extracted features or labels not found in parsed data.\")\n\n#     print(\"\\n--- Feature Extraction and Label Integration Complete. Building DataFrame. ---\")\n\n#     if not all_videos_features:\n#         print(\"No features extracted from any video. Exiting the ML pipeline.\")\n#     else:\n#         # Create a Pandas DataFrame from all collected per-frame features\n#         df_full_dataset = pd.DataFrame(all_videos_features)\n        \n#         # --- Data Cleaning/Filtering for ML ---\n#         # Filter out frames with no valid emotion label for the emotion prediction stage.\n#         df_emotion_data = df_full_dataset[df_full_dataset['emotion_label_str'] != 'No_Emotion_Label_Parsed'].copy()\n#         df_emotion_data = df_emotion_data[df_emotion_data['emotion_label_str'] != 'nothing_emotion'].copy()\n        \n#         # Filter out frames with no valid behavior label for the behavior prediction stage.\n#         df_behavior_data = df_full_dataset[df_full_dataset['behavior_label_str'] != 'No_Behavior_Label_Parsed'].copy()\n#         df_behavior_data = df_behavior_data[df_behavior_data['behavior_label_str'] != 'nothing_behavior'].copy()\n\n#         # Define the common motion feature columns used as input to both models\n#         motion_feature_columns = [\n#             'avg_crowd_speed', 'dominant_crowd_angle', 'avg_flow_dx', 'avg_flow_dy',\n#             'flow_magnitude_variance', 'directional_coherence', 'confusion_index',\n#             'total_crowd_count', 'avg_density_in_mask', 'enthalpy',\n#             'flow_divergence', 'flow_vorticity'\n#         ]\n        \n#         # --- Stage 1: Train Emotion Classifier (Motion Features -> Emotion Labels) ---\n#         print(\"\\n--- Stage 1: Training Emotion Classifier (Motion Features -> Emotion Labels) ---\")\n#         if not df_emotion_data.empty:\n#             X_emotion = df_emotion_data[motion_feature_columns]\n#             y_emotion = df_emotion_data['emotion_label_str']\n\n#             emotion_label_encoder = LabelEncoder()\n#             y_emotion_encoded = emotion_label_encoder.fit_transform(y_emotion)\n            \n#             # Split emotion data\n#             X_emotion_train, X_emotion_temp, y_emotion_train, y_emotion_temp = train_test_split(\n#                 X_emotion, y_emotion_encoded, test_size=0.3, random_state=42, stratify=y_emotion_encoded)\n#             X_emotion_val, X_emotion_test, y_emotion_val, y_emotion_test = train_test_split(\n#                 X_emotion_temp, y_emotion_temp, test_size=0.5, random_state=42, stratify=y_emotion_temp)\n\n#             # Scale emotion features\n#             emotion_scaler = StandardScaler()\n#             X_emotion_train_scaled = emotion_scaler.fit_transform(X_emotion_train)\n#             X_emotion_val_scaled = emotion_scaler.transform(X_emotion_val)\n#             X_emotion_test_scaled = emotion_scaler.transform(X_emotion_test)\n\n#             # Train Emotion Model\n#             emotion_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n#             emotion_model.fit(X_emotion_train_scaled, y_emotion_train)\n#             print(\"Emotion Classifier Training Complete.\")\n\n#             # Evaluate Emotion Model\n#             y_emotion_val_pred = emotion_model.predict(X_emotion_val_scaled)\n#             print(\"\\n--- Emotion Classifier Validation Set Performance ---\")\n#             print(f\"Accuracy: {accuracy_score(y_emotion_val, y_emotion_val_pred):.4f}\")\n#             print(\"Classification Report:\\n\", classification_report(y_emotion_val, y_emotion_val_pred, target_names=emotion_label_encoder.classes_))\n#             print(\"Confusion Matrix:\\n\", confusion_matrix(y_emotion_val, y_emotion_val_pred))\n            \n#             y_emotion_test_pred = emotion_model.predict(X_emotion_test_scaled)\n#             print(\"\\n--- Emotion Classifier Test Set Performance ---\")\n#             print(f\"Accuracy: {accuracy_score(y_emotion_test, y_emotion_test_pred):.4f}\")\n#             print(\"Classification Report:\\n\", classification_report(y_emotion_test, y_emotion_test_pred, target_names=emotion_label_encoder.classes_))\n#             print(\"Confusion Matrix:\\n\", confusion_matrix(y_emotion_test, y_emotion_test_pred))\n\n#         else:\n#             print(\"No data available for emotion prediction. Skipping Stage 1 (Emotion Classifier).\")\n#             emotion_model = None # Ensure emotion_model is None if not trained\n\n#         # --- Stage 2: Train Behavior Classifier (Motion Features + Predicted Emotion Probabilities -> Behavior Labels) ---\n#         print(\"\\n--- Stage 2: Training Behavior Classifier (Motion Features + Inferred Emotion -> Behavior Labels) ---\")\n#         if not df_behavior_data.empty and emotion_model is not None:\n#             # We need to ensure X_behavior_motion_raw is aligned with df_behavior_data's indices\n#             # and that it's transformed using the *same scaler* as emotion model was trained on.\n#             # Make sure we use the original (unscaled) motion features from df_behavior_data\n#             # when predicting emotion probabilities, then scale the *combined* features for behavior model.\n\n#             # Step 1: Predict emotion probabilities using the trained emotion model\n#             # and the *original motion features* for the behavior dataset's frames.\n#             # Important: Use the same scaler that was fitted on the emotion training data.\n#             X_behavior_motion_for_emotion_pred = emotion_scaler.transform(df_behavior_data[motion_feature_columns])\n#             predicted_emotion_probabilities = emotion_model.predict_proba(X_behavior_motion_for_emotion_pred)\n\n#             # Convert predicted probabilities to a DataFrame and add to the main feature set\n#             predicted_emotion_df = pd.DataFrame(\n#                 predicted_emotion_probabilities,\n#                 columns=[f'pred_emotion_prob_{cls}' for cls in emotion_label_encoder.classes_],\n#                 index=df_behavior_data.index # Crucial: preserve original index for alignment\n#             )\n            \n#             # Combine original motion features with predicted emotion probabilities\n#             X_behavior_combined = pd.concat([df_behavior_data[motion_feature_columns], predicted_emotion_df], axis=1)\n#             y_behavior = df_behavior_data['behavior_label_str'] # Target labels for behavior\n\n#             behavior_label_encoder = LabelEncoder()\n#             y_behavior_encoded = behavior_label_encoder.fit_transform(y_behavior)\n\n#             # Split behavior data using indices to ensure consistent splits across X and y\n#             train_idx, temp_idx, _, _ = train_test_split(\n#                 X_behavior_combined.index, y_behavior_encoded, test_size=0.3, random_state=42, stratify=y_behavior_encoded)\n#             val_idx, test_idx, _, _ = train_test_split(\n#                 temp_idx, y_behavior_encoded[temp_idx], test_size=0.5, random_state=42, stratify=y_behavior_encoded[temp_idx])\n\n#             # Select data using the generated indices\n#             X_behavior_train_combined = X_behavior_combined.loc[train_idx]\n#             y_behavior_train = y_behavior_encoded[train_idx]\n            \n#             X_behavior_val_combined = X_behavior_combined.loc[val_idx]\n#             y_behavior_val = y_behavior_encoded[val_idx]\n\n#             X_behavior_test_combined = X_behavior_combined.loc[test_idx]\n#             y_behavior_test = y_behavior_encoded[test_idx]\n            \n#             # Scale the combined features for the behavior model. A new scaler is used.\n#             behavior_scaler = StandardScaler()\n#             X_behavior_train_scaled = behavior_scaler.fit_transform(X_behavior_train_combined)\n#             X_behavior_val_scaled = behavior_scaler.transform(X_behavior_val_combined)\n#             X_behavior_test_scaled = behavior_scaler.transform(X_behavior_test_combined)\n\n#             # Train Behavior Model\n#             behavior_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n#             behavior_model.fit(X_behavior_train_scaled, y_behavior_train)\n#             print(\"Behavior Classifier Training Complete.\")\n\n#             # Evaluate Behavior Model\n#             y_behavior_val_pred = behavior_model.predict(X_behavior_val_scaled)\n#             print(\"\\n--- Behavior Classifier Validation Set Performance ---\")\n#             print(f\"Accuracy: {accuracy_score(y_behavior_val, y_behavior_val_pred):.4f}\")\n#             print(\"Classification Report:\\n\", classification_report(y_behavior_val, y_behavior_val_pred, target_names=behavior_label_encoder.classes_))\n#             print(\"Confusion Matrix:\\n\", confusion_matrix(y_behavior_val, y_behavior_val_pred))\n            \n#             y_behavior_test_pred = behavior_model.predict(X_behavior_test_scaled)\n#             print(\"\\n--- Behavior Classifier Test Set Performance ---\")\n#             print(f\"Accuracy: {accuracy_score(y_behavior_test, y_behavior_test_pred):.4f}\")\n#             print(\"Classification Report:\\n\", classification_report(y_behavior_test, y_behavior_test_pred, target_names=behavior_label_encoder.classes_))\n#             print(\"Confusion Matrix:\\n\", confusion_matrix(y_behavior_test, y_behavior_test_pred))\n\n#         else:\n#             print(\"No data available for behavior prediction or emotion model not trained. Skipping Stage 2 (Behavior Classifier).\")\n\n#     print(\"\\n--- Overall Crowd Behavior Analysis Pipeline Complete ---\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Define dataset paths (if you need to iterate through multiple videos later)\n    # dataset_base_path = \"/kaggle/input/motion-dataset/\"\n    # video_dir = os.path.join(dataset_base_path, \"Motion_Emotion_Dataset\")\n    \n    # Output directory for generated videos and collected feature data\n    kaggle_output_base_dir = \"/kaggle/working/\"\n    output_analysis_videos_dir = os.path.join(kaggle_output_base_dir, \"crowd_motion_analysis_results\")\n    os.makedirs(output_analysis_videos_dir, exist_ok=True)\n\n    print(\"\\n--- Starting Crowd Motion Feature Extraction ---\")\n\n    all_extracted_features = [] # List to collect features from all videos\n\n    # Hardcoded video path as per your request for a single video\n    video_path_to_process = '/kaggle/input/new-dataset/1.mp4'\n    \n        # Call the main analysis function with segmented output enabled\n    per_frame_features = analyze_crowd_motion_in_video(\n        video_path=video_path_to_process,\n        output_video_dir=output_analysis_videos_dir,\n        density_threshold_for_refinement=0.2,\n        ema_alpha=0.2,\n        max_frames_to_process=None, # Process full video\n        frames_per_segment=50 # Save video in 50-frame segments\n        )\n        \n    if per_frame_features:\n        all_extracted_features.extend(per_frame_features)\n        print(f\"Extracted {len(per_frame_features)} frames of motion features for {video_file_name_base}.\")\n    else:\n        print(f\"No motion features extracted for {video_file_name_base}. It might be corrupted or empty.\")\n\n    print(\"\\n--- All Video Processing Complete ---\")\n    print(f\"Total frames with features extracted across all videos: {len(all_extracted_features)}\")\n\n    # If you later want to convert this list to a DataFrame or save it:\n    # import pandas as pd\n    # df_all_features = pd.DataFrame(all_extracted_features)\n    # csv_output_path = os.path.join(kaggle_output_base_dir, \"all_crowd_motion_features.csv\")\n    # df_all_features.to_csv(csv_output_path, index=False)\n    # print(f\"All extracted features saved to: {csv_output_path}\")\n\n    print(\"\\nMotion feature extraction pipeline finished. Segmented output videos are saved in the 'crowd_motion_analysis_results' directory.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}