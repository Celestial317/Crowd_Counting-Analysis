{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12188377,"sourceType":"datasetVersion","datasetId":7677091}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport os\nfrom PIL import Image\nimport numpy as np\nimport scipy.io","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:09:46.786753Z","iopub.execute_input":"2025-06-18T17:09:46.786956Z","iopub.status.idle":"2025-06-18T17:09:54.974570Z","shell.execute_reply.started":"2025-06-18T17:09:46.786938Z","shell.execute_reply":"2025-06-18T17:09:54.973864Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"IMG_HEIGHT = 256 \nIMG_WIDTH = 256 \nBATCH_SIZE = 8\nPATCH_SIZE = 64\nNUM_ROUTER_CLASSES = 3\n\nTRAIN_IMAGE_DIR = '/kaggle/input/crowd-dataset/crowd_wala_dataset/train_data/images'\nTRAIN_GT_DIR = '/kaggle/input/crowd-dataset/crowd_wala_dataset/train_data/ground_truth'\nTEST_IMAGE_DIR = '/kaggle/input/crowd-dataset/crowd_wala_dataset/test_data/images'\nTEST_GT_DIR = '/kaggle/input/crowd-dataset/crowd_wala_dataset/test_data/ground_truth'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:10:48.725414Z","iopub.execute_input":"2025-06-18T17:10:48.726021Z","iopub.status.idle":"2025-06-18T17:10:48.729834Z","shell.execute_reply.started":"2025-06-18T17:10:48.725995Z","shell.execute_reply":"2025-06-18T17:10:48.729228Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:10:50.449968Z","iopub.execute_input":"2025-06-18T17:10:50.450273Z","iopub.status.idle":"2025-06-18T17:10:50.454601Z","shell.execute_reply.started":"2025-06-18T17:10:50.450253Z","shell.execute_reply":"2025-06-18T17:10:50.453839Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"if not os.path.exists(TRAIN_IMAGE_DIR):\n    print(f\"Warning: {TRAIN_IMAGE_DIR} does not exist.\")\nif not os.path.exists(TRAIN_GT_DIR):\n    print(f\"Warning: {TRAIN_GT_DIR} does not exist.\")\nif not os.path.exists(TEST_IMAGE_DIR):\n    print(f\"Warning: {TEST_IMAGE_DIR} does not exist.\")\nif not os.path.exists(TEST_GT_DIR):\n    print(f\"Warning: {TEST_GT_DIR} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:10:50.585864Z","iopub.execute_input":"2025-06-18T17:10:50.586568Z","iopub.status.idle":"2025-06-18T17:10:50.626280Z","shell.execute_reply.started":"2025-06-18T17:10:50.586542Z","shell.execute_reply":"2025-06-18T17:10:50.625423Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class CustomCrowdDataset(Dataset):\n    def __init__(self, img_folder_path, gt_folder_path, img_height, img_width, transform=None):\n        self.img_folder_path = img_folder_path\n        self.gt_folder_path = gt_folder_path\n        self.img_height = img_height\n        self.img_width = img_width\n        self.transform = transform\n        self.image_filenames = sorted([f for f in os.listdir(img_folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_filename = self.image_filenames[idx]\n        img_path = os.path.join(self.img_folder_path, img_filename)\n        \n        base_filename = os.path.splitext(img_filename)[0]\n        gt_filename = f\"GT_{base_filename}.mat\" \n        gt_path = os.path.join(self.gt_folder_path, gt_filename)\n\n        if not os.path.exists(gt_path):\n            print(f\"Warning: GT not found for {img_filename} (expected {gt_path}). Returning dummy data. ðŸ§Š\")\n            img = Image.new('RGB', (self.img_width, self.img_height), color='black')\n            dummy_img = self.transform(img) if self.transform else transforms.ToTensor()(img)\n            return dummy_img, torch.tensor(0.0, dtype=torch.float32)\n\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n            img = img.resize((self.img_width, self.img_height)) \n\n            data = scipy.io.loadmat(gt_path)\n            \n            count = 0\n            if 'image_info' in data and data['image_info'].size > 0:\n                info_entry = data['image_info'][0,0] \n                \n                try:\n                    points_data = info_entry[0][0][0][0][0] \n                    \n                    if isinstance(points_data, np.ndarray):\n                        if points_data.size > 0:\n                            count = points_data.shape[0]\n                        else:\n                            count = 0 \n                    elif isinstance(points_data, (int, float, np.int_, np.float_)):\n                        count = float(points_data)\n                    else:\n                        print(f\"Warning: Unexpected data type after deep access for {gt_filename}: {type(points_data)}. Setting count to 0\")\n\n                except (IndexError, KeyError, TypeError) as e:\n                    print(f\"Error accessing nested 'points' in 'image_info' for {gt_filename}: {e}.\")\n                    print(f\"Please confirm the exact nested structure if not 'image_info[0,0][0][0][0][0]'.\")\n                    print(\"Setting count to 0.\")\n                    count = 0\n            else:\n                print(f\"Warning: 'image_info' not found or is empty in {gt_filename}.\")\n                print(f\"Available top-level keys in {gt_filename}: {data.keys()}\")\n                print(\"Setting count to 0.\")\n                count = 0\n            \n            if self.transform:\n                img = self.transform(img)\n\n            return img, torch.tensor(float(count), dtype=torch.float32)\n\n        except Exception as e:\n            print(f\"Error processing {img_filename} or its GT: {e}. Returning dummy data. ðŸ’¥\")\n            img = Image.new('RGB', (self.img_width, self.img_height), color='black')\n            dummy_img = self.transform(img) if self.transform else transforms.ToTensor()(img)\n            return dummy_img, torch.tensor(0.0, dtype=torch.float32)\n\nimage_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = CustomCrowdDataset(TRAIN_IMAGE_DIR, TRAIN_GT_DIR, IMG_HEIGHT, IMG_WIDTH, transform=image_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\ntest_dataset = CustomCrowdDataset(TEST_IMAGE_DIR, TEST_GT_DIR, IMG_HEIGHT, IMG_WIDTH, transform=image_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(f\"Training set size: {len(train_dataset)} images\")\nprint(f\"Test set size: {len(test_dataset)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:10:52.583253Z","iopub.execute_input":"2025-06-18T17:10:52.583525Z","iopub.status.idle":"2025-06-18T17:10:52.642350Z","shell.execute_reply.started":"2025-06-18T17:10:52.583505Z","shell.execute_reply":"2025-06-18T17:10:52.641586Z"}},"outputs":[{"name":"stdout","text":"Training set size: 400 images\nTest set size: 316 images\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def get_patches(image_tensor, patch_size):\n    assert image_tensor.shape[1] % patch_size == 0, \n    assert image_tensor.shape[2] % patch_size == 0, \n\n    patches = image_tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n\n    patches = patches.permute(1, 2, 0, 3, 4).contiguous()\n    patches = patches.view(-1, image_tensor.shape[0], patch_size, patch_size)\n    \n    return patches\n\nclass RouterNetwork(nn.Module):\n    def __init__(self, in_channels, num_router_classes):\n        super(RouterNetwork, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) \n\n        self.fc1 = nn.Linear(64 * (PATCH_SIZE // 4) * (PATCH_SIZE // 4), 128)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(128, num_router_classes) \n\n    def forward(self, x):\n        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n        x = x.view(x.size(0), -1) \n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x \n\n\nclass MCNNColumn(nn.Module):\n\n    def __init__(self, in_channels):\n        super(MCNNColumn, self).__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_channels, 16, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2), \n            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2) \n        )\n\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(in_channels, 20, kernel_size=7, padding=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2), \n            nn.Conv2d(20, 40, kernel_size=7, padding=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2) \n        )\n\n        self.branch3 = nn.Sequential(\n            nn.Conv2d(in_channels, 24, kernel_size=9, padding=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            nn.Conv2d(24, 48, kernel_size=9, padding=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2) \n        )\n\n        self.merge_conv = nn.Conv2d(32 + 40 + 48, 80, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n\n        self.global_pool = nn.AdaptiveAvgPool2d(1) \n        self.final_fc = nn.Linear(80, 1) \n\n    def forward(self, x):\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        x = torch.cat((x1, x2, x3), dim=1)\n        x = self.relu(self.merge_conv(x))\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1) \n        x = self.final_fc(x) \n        return x ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:10:54.580566Z","iopub.execute_input":"2025-06-18T17:10:54.580835Z","iopub.status.idle":"2025-06-18T17:10:54.594570Z","shell.execute_reply.started":"2025-06-18T17:10:54.580814Z","shell.execute_reply":"2025-06-18T17:10:54.593744Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SwitchMCNN(nn.Module):\n\n    def __init__(self, in_channels=3, patch_size=PATCH_SIZE, num_router_classes=NUM_ROUTER_CLASSES):\n        super(SwitchMCNN, self).__init__()\n        self.patch_size = patch_size\n        self.num_router_classes = num_router_classes\n\n        self.router = RouterNetwork(in_channels, num_router_classes)\n    \n        self.mcnn_columns = nn.ModuleList([MCNNColumn(in_channels) for _ in range(num_router_classes)])\n\n    def forward(self, x):\n        batch_size, channels, img_h, img_w = x.shape\n\n        num_patches_h = img_h // self.patch_size\n        num_patches_w = img_w // self.patch_size\n        total_patches_per_image = num_patches_h * num_patches_w\n\n        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n \n        patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n\n        patches_flat = patches.view(-1, channels, self.patch_size, self.patch_size)\n\n        router_logits = self.router(patches_flat)\n\n        a, routed_columns_indices = torch.max(router_logits, dim=1)\n\n        final_counts = torch.zeros(batch_size, dtype=torch.float32).to(x.device)\n\n        for i in range(batch_size):\n            image_patches_start_idx = i * total_patches_per_image\n            image_patches_end_idx = (i + 1) * total_patches_per_image\n            \n            current_image_patches = patches_flat[image_patches_start_idx : image_patches_end_idx]\n            current_image_routed_indices = routed_columns_indices[image_patches_start_idx : image_patches_end_idx]\n\n            image_total_count = torch.tensor(0.0).to(x.device)\n\n            for patch_idx in range(total_patches_per_image):\n                patch = current_image_patches[patch_idx].unsqueeze(0) \n                assigned_column_index = current_image_routed_indices[patch_idx].item()\n\n                mcnn_column = self.mcnn_columns[assigned_column_index]\n\n                patch_count_prediction = mcnn_column(patch)\n                image_total_count += patch_count_prediction.squeeze()\n\n            final_counts[i] = image_total_count\n            \n        return final_counts.unsqueeze(1) \n\nmodel = SwitchMCNN(in_channels=3, patch_size=PATCH_SIZE, num_router_classes=NUM_ROUTER_CLASSES).to(device)\n\ncriterion = nn.L1Loss() \noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # Learning rate might need tuning\n\nNUM_EPOCHS = 3\ndef train_model(model, train_loader, criterion, optimizer, num_epochs):\n    model.train() \n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, (images, counts) in enumerate(train_loader):\n            images = images.to(device)\n            counts = counts.unsqueeze(1).to(device) \n\n            optimizer.zero_grad()\n\n            outputs = model(images)\n            loss = criterion(outputs, counts)\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n\n            if (i + 1) % 50 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {epoch_loss:.4f}\")\n\nif __name__ == '__main__':\n    print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters.\")\n    train_model(model, train_loader, criterion, optimizer, NUM_EPOCHS)\n    model_path = 'switch_mcnn_crowd_counter.pth'\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved to {model_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:18:41.603270Z","iopub.execute_input":"2025-06-18T17:18:41.604040Z","iopub.status.idle":"2025-06-18T17:19:26.901377Z","shell.execute_reply.started":"2025-06-18T17:18:41.604006Z","shell.execute_reply":"2025-06-18T17:19:26.900625Z"}},"outputs":[{"name":"stdout","text":"Model initialized with 2612926 trainable parameters.\nStarting training...\nEpoch [1/3], Step [50/50], Loss: 68.6814\nEpoch [1/3] completed. Average Loss: 236.9926\nEpoch [2/3], Step [50/50], Loss: 227.8536\nEpoch [2/3] completed. Average Loss: 199.1722\nEpoch [3/3], Step [50/50], Loss: 157.1553\nEpoch [3/3] completed. Average Loss: 196.9798\nModel saved to switch_mcnn_crowd_counter.pth\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def evaluate_model(model, test_loader, criterion):\n    model.eval()\n    total_loss = 0.0\n    total_mae = 0.0 \n    total_mse = 0.0 \n    total_samples = 0\n    \n    print(\"Starting evaluation...\")\n    with torch.no_grad(): \n        for images, counts in test_loader:\n            images = images.to(device)\n            counts = counts.unsqueeze(1).to(device)\n\n            outputs = model(images)\n\n            loss = criterion(outputs, counts)\n            total_loss += loss.item() * images.size(0)\n\n            mae_batch = torch.abs(outputs - counts).sum()\n            total_mae += mae_batch.item()\n\n            mse_batch = F.mse_loss(outputs, counts, reduction='sum')\n            total_mse += mse_batch.item()\n            \n            total_samples += images.size(0)\n\n    avg_loss = total_loss / total_samples\n    avg_mae = total_mae / total_samples\n    avg_mse = total_mse / total_samples \n\n    print(f\"Evaluation Complete.\")\n    print(f\"Average MAE: {avg_mae:.4f}\")\n    print(f\"Average MSE: {avg_mse:.4f}\")\n    \n    return avg_loss, avg_mae, avg_mse \n\n\nif __name__ == '__main__':\n\n    loaded_model = SwitchMCNN(in_channels=3, patch_size=PATCH_SIZE, num_router_classes=NUM_ROUTER_CLASSES).to(device)\n\n    model_path = 'switch_mcnn_crowd_counter.pth'\n    \n    try:\n\n        loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n        print(f\"Model loaded successfully from {model_path}\")\n\n        criterion_eval = nn.L1Loss() \n\n        evaluate_model(loaded_model, test_loader, criterion_eval)\n\n    except FileNotFoundError:\n        print(f\"Error: Model file not found at {model_path}. Please ensure the training block was run and the model was saved.\")\n    except Exception as e:\n        print(f\"An error occurred during model loading or evaluation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:19:33.834166Z","iopub.execute_input":"2025-06-18T17:19:33.834473Z","iopub.status.idle":"2025-06-18T17:19:40.580077Z","shell.execute_reply.started":"2025-06-18T17:19:33.834447Z","shell.execute_reply":"2025-06-18T17:19:40.579016Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully from switch_mcnn_crowd_counter.pth\nStarting evaluation...\nEvaluation Complete.\nAverage MAE: 169.0175\nAverage MSE: 60315.3825\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}